"""
Comprehensive CUDA/GPU Usage Analysis Report for TokenSHAP-SFA Project
"""

import sys
import os
sys.path.append('.')

def analyze_cuda_usage():
    """Generate comprehensive CUDA usage analysis"""
    
    print("ğŸ” CUDA/GPU Usage Analysis Report")
    print("=" * 50)
    
    # Check current environment
    print("\nğŸ“‹ ENVIRONMENT STATUS")
    print("-" * 25)
    
    try:
        import torch
        torch_available = True
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"ğŸ¯ CUDA Available: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"ğŸ–¥ï¸  CUDA Version: {torch.version.cuda}")
            print(f"ğŸ“± Device Count: {torch.cuda.device_count()}")
            print(f"ğŸ·ï¸  Device Name: {torch.cuda.get_device_name(0)}")
        else:
            print("âš ï¸  CUDA not available - CPU mode only")
    except ImportError:
        torch_available = False
        print("âŒ PyTorch not installed - using CPU-only implementations")
    
    try:
        from transformers import __version__ as transformers_version
        print(f"ğŸ¤— Transformers: {transformers_version}")
    except ImportError:
        print("âŒ Transformers not installed")
    
    # Analyze configuration
    print(f"\nâš™ï¸  CONFIGURATION ANALYSIS")
    print("-" * 30)
    
    try:
        from config import TokenSHAPConfig, TORCH_AVAILABLE
        config = TokenSHAPConfig()
        print(f"ğŸ”§ TORCH_AVAILABLE flag: {TORCH_AVAILABLE}")
        print(f"ğŸ® use_gpu setting: {config.use_gpu}")
        print(f"ğŸ­ parallel_workers: {config.parallel_workers}")
        
        if not TORCH_AVAILABLE:
            print("â„¹ï¸  Config automatically disabled GPU due to missing PyTorch")
        elif not config.use_gpu:
            print("â„¹ï¸  Config disabled GPU usage")
    except Exception as e:
        print(f"âŒ Config analysis failed: {str(e)}")
    
    # Analyze source code GPU usage
    print(f"\nğŸ“ SOURCE CODE ANALYSIS")
    print("-" * 28)
    
    gpu_usage_files = {
        "token_shap.py": {
            "description": "Enhanced TokenSHAP Implementation",
            "gpu_features": [
                "Lines 35-36: model.cuda() - Moves model to GPU",
                "Lines 49-50: inputs = {k: v.cuda() for k, v in inputs.items()} - Moves input tensors to GPU",
                "GPU usage conditional on config.use_gpu and torch.cuda.is_available()"
            ],
            "cpu_fallback": "âœ… Yes - operates on CPU if CUDA unavailable"
        },
        
        "cot_explainer.py": {
            "description": "Chain-of-Thought TokenSHAP",
            "gpu_features": [
                "Lines 67-68: inputs = {k: v.cuda() for k, v in inputs.items()} - GPU tensor operations",
                "Uses same GPU logic as token_shap.py"
            ],
            "cpu_fallback": "âœ… Yes - falls back to CPU computation"
        },
        
        "config.py": {
            "description": "Configuration Management",
            "gpu_features": [
                "Line 47: use_gpu = TORCH_AVAILABLE and torch.cuda.is_available()",
                "Smart detection of GPU availability",
                "Automatic fallback to CPU if CUDA unavailable"
            ],
            "cpu_fallback": "âœ… Yes - automatic CPU fallback"
        },
        
        "tokenshap_ollama.py": {
            "description": "Ollama Integration (CPU-Only)",
            "gpu_features": [
                "NO GPU usage - designed for Ollama API",
                "Uses SimpleTokenizer instead of transformers",
                "Pure CPU implementation"
            ],
            "cpu_fallback": "âœ… CPU-only by design"
        },
        
        "cot_ollama_reasoning.py": {
            "description": "Ollama CoT Analysis (CPU-Only)",
            "gpu_features": [
                "NO GPU usage - Ollama API based",
                "CPU-only token processing",
                "No PyTorch dependencies"
            ],
            "cpu_fallback": "âœ… CPU-only by design"
        }
    }
    
    for filename, info in gpu_usage_files.items():
        print(f"\nğŸ“„ {filename}")
        print(f"   ğŸ“‹ {info['description']}")
        print(f"   ğŸ® GPU Features:")
        for feature in info['gpu_features']:
            print(f"      â€¢ {feature}")
        print(f"   ğŸ”„ CPU Fallback: {info['cpu_fallback']}")
    
    # Analyze current deployment
    print(f"\nğŸš€ CURRENT DEPLOYMENT STATUS")
    print("-" * 35)
    
    if not torch_available:
        print("ğŸ“ STATUS: CPU-Only Mode (PyTorch not installed)")
        print("ğŸ”§ ACTIVE: Ollama-based implementations only")
        print("âš¡ PERFORMANCE: Using lightweight CPU implementations")
        print("ğŸ’¡ RECOMMENDATION: This is optimal for Ollama usage")
        
        active_components = [
            "âœ… OllamaCoTAnalyzer - CoT analysis with phi4-reasoning",
            "âœ… TokenSHAPWithOllama - CPU-based token attribution", 
            "âœ… SimpleTokenizer - Basic tokenization",
            "âœ… SFAMetaLearner - Scikit-learn based meta-learning",
            "âŒ EnhancedTokenSHAP - Requires PyTorch",
            "âŒ CoTTokenSHAP - Requires transformers"
        ]
        
    else:
        try:
            import torch
            if torch.cuda.is_available():
                print("ğŸ“ STATUS: GPU-Enabled Mode")
                print("ğŸ® GPU AVAILABLE: Full CUDA acceleration possible")
            else:
                print("ğŸ“ STATUS: CPU Mode (PyTorch installed, no CUDA)")
                print("ğŸ”§ FALLBACK: Using CPU tensors")
        except:
            pass
            
        active_components = [
            "âœ… All components available",
            "âœ… GPU acceleration when enabled",
            "âœ… Automatic CPU fallback"
        ]
    
    print(f"\nğŸ—ï¸  ACTIVE COMPONENTS:")
    for component in active_components:
        print(f"   {component}")
    
    # Performance implications
    print(f"\nâš¡ PERFORMANCE IMPLICATIONS")
    print("-" * 32)
    
    performance_analysis = {
        "With CUDA (if available)": [
            "ğŸš€ Faster model inference for transformers",
            "âš¡ GPU-accelerated tensor operations",
            "ğŸ¯ Better performance for large models",
            "ğŸ’¾ Higher memory usage (GPU VRAM)"
        ],
        "CPU-Only Mode (current)": [
            "ğŸ”§ Works with Ollama models perfectly",
            "ğŸ’¡ Lower memory requirements", 
            "ğŸŒ No CUDA driver dependencies",
            "â° Slower for large transformer models",
            "âœ… Optimal for your current Ollama setup"
        ]
    }
    
    for mode, implications in performance_analysis.items():
        print(f"\nğŸ“Š {mode}:")
        for implication in implications:
            print(f"   {implication}")
    
    # Recommendations
    print(f"\nğŸ’¡ RECOMMENDATIONS")
    print("-" * 20)
    
    if not torch_available:
        print("ğŸ¯ CURRENT SETUP IS OPTIMAL FOR OLLAMA:")
        print("   âœ… No unnecessary dependencies")
        print("   âœ… Lightweight CPU implementations")
        print("   âœ… Perfect for phi4-reasoning analysis")
        print("   âœ… SFA meta-learning works great on CPU")
        print("")
        print("ğŸ”® IF YOU WANT FULL TRANSFORMER SUPPORT:")
        print("   ğŸ“¦ Install: conda install pytorch transformers")
        print("   ğŸ® For GPU: conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia")
    else:
        print("ğŸ¯ CURRENT SETUP HAS FULL CAPABILITIES")
        print("   âœ… Can use both Ollama and transformers")
        print("   âœ… GPU acceleration available if needed")
        print("   âœ… Automatic fallback to CPU")
    
    print(f"\n" + "=" * 60)
    print("ğŸ“‹ SUMMARY")
    print("=" * 60)
    
    current_mode = "CPU-Only (Ollama Optimized)" if not torch_available else "Full PyTorch Support"
    
    summary = f"""
ğŸ·ï¸  Current Mode: {current_mode}
ğŸ® GPU Usage: {'âŒ Not available' if not torch_available else 'âœ… Available with fallback'}
ğŸš€ Ollama Integration: âœ… Fully functional
ğŸ§  CoT Analysis: âœ… Working with phi4-reasoning
âš¡ Performance: {'Optimized for Ollama' if not torch_available else 'Full acceleration available'}

ğŸ¯ Your codebase intelligently adapts to available hardware:
   â€¢ CPU-only when PyTorch unavailable (current)
   â€¢ GPU-accelerated when CUDA available
   â€¢ Automatic fallback ensures compatibility
   â€¢ Ollama integration always works regardless
"""
    
    print(summary)
    print("=" * 60)

def check_specific_gpu_usage():
    """Check specific GPU usage in current session"""
    print("\nğŸ” RUNTIME GPU USAGE CHECK")
    print("-" * 30)
    
    try:
        from config import TokenSHAPConfig
        config = TokenSHAPConfig()
        print(f"ğŸ“‹ Current use_gpu setting: {config.use_gpu}")
        
        if hasattr(config, 'use_gpu') and config.use_gpu:
            print("âš ï¸  GPU is enabled in config but may not be used due to missing PyTorch")
        else:
            print("âœ… GPU disabled - using CPU implementations")
            
    except Exception as e:
        print(f"âŒ Could not check config: {str(e)}")

if __name__ == "__main__":
    analyze_cuda_usage()
    check_specific_gpu_usage()